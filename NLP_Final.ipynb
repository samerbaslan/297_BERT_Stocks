{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpFlrJk0Hdj6",
        "outputId": "f09fc63c-16e6-450d-f030-931ffb7b5169"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlugh9coM-JD"
      },
      "source": [
        "!pip install vaderSentiment -qqq\n",
        "!pip install transformers==2.11.0 -qqq"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v1U3lgAMf07",
        "outputId": "9b3b94f8-147e-4ac1-a683-677043207dc0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid')\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "import json\n",
        "import string, collections, unicodedata\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "XAtDi7PXMzjG",
        "outputId": "59ff626b-fb43-481f-f201-49a82b12cb85"
      },
      "source": [
        "news_df = pd.read_csv(\"/content/drive/MyDrive/297_BERT_Stocks/data/news/all_news\")\n",
        "news_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company_name</th>\n",
              "      <th>title</th>\n",
              "      <th>article</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>apple</td>\n",
              "      <td>New York City Mayor-Elect Eric Adams says he w...</td>\n",
              "      <td>New York City Mayor-Elect Eric Adams took to T...</td>\n",
              "      <td>Business Insider</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apple</td>\n",
              "      <td>Zia Credit Union review: Hispanic American-led...</td>\n",
              "      <td>Personal Finance Insider writes about products...</td>\n",
              "      <td>Business Insider</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>apple</td>\n",
              "      <td>10 things in tech you need to know</td>\n",
              "      <td>Hello, world. We've got news about a new way t...</td>\n",
              "      <td>Business Insider</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>apple</td>\n",
              "      <td>5 ways I'm using credit card points to cover 7...</td>\n",
              "      <td>This post contains links to products from our ...</td>\n",
              "      <td>Business Insider</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>apple</td>\n",
              "      <td>Sen. Mitt Romney dresses up as Ted Lasso in Ha...</td>\n",
              "      <td>Sen. Mitt Romney of Utah donned one of this ye...</td>\n",
              "      <td>Business Insider</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>netflix</td>\n",
              "      <td>Hollywood Union Approves Contracts With Studio...</td>\n",
              "      <td>One of Hollywoods most powerful unions approve...</td>\n",
              "      <td>Bloomberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>netflix</td>\n",
              "      <td>South Korea's Moment Owes More to Samsung Than...</td>\n",
              "      <td>South Korea is having a moment. Between the hi...</td>\n",
              "      <td>Bloomberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>netflix</td>\n",
              "      <td>Disney+ is launching a special IMAX format for...</td>\n",
              "      <td>Disney+ is bringing IMAXs Expanded Aspect Rati...</td>\n",
              "      <td>TechCrunch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>netflix</td>\n",
              "      <td>When Amazon Buys MGM, Hollywood Fears the Loss...</td>\n",
              "      <td>Where Hollywood and Silicon Valley collildeGet...</td>\n",
              "      <td>Bloomberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>netflix</td>\n",
              "      <td>TechCrunch+ roundup: Yahoo leaves China, Nuban...</td>\n",
              "      <td>After Nubank filed its F-1, Natasha Mascarenha...</td>\n",
              "      <td>TechCrunch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>156 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    company_name  ...            source\n",
              "0          apple  ...  Business Insider\n",
              "1          apple  ...  Business Insider\n",
              "2          apple  ...  Business Insider\n",
              "3          apple  ...  Business Insider\n",
              "4          apple  ...  Business Insider\n",
              "..           ...  ...               ...\n",
              "151      netflix  ...         Bloomberg\n",
              "152      netflix  ...         Bloomberg\n",
              "153      netflix  ...        TechCrunch\n",
              "154      netflix  ...         Bloomberg\n",
              "155      netflix  ...        TechCrunch\n",
              "\n",
              "[156 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOTrJesBPvQU"
      },
      "source": [
        "with open('/content/drive/MyDrive/297_BERT_Stocks/common_word_phrases.json', 'r') as f:\n",
        "    phrases_dict = json.load(f)\n",
        "phrases = phrases_dict['phrases']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAJ5N3YpQS4m"
      },
      "source": [
        "stop_words = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
        "             \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\n",
        "             \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
        "             \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\",\n",
        "             \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "             \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
        "             \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
        "             \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\",\n",
        "             \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
        "             \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\",\n",
        "             \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\",\n",
        "             \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\",\n",
        "             \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
        "             \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
        "             \"can\", \"will\", \"just\", \"should\", \"now\", \"not\"}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4whDtbZ-O-j7"
      },
      "source": [
        "# Custom lemmatize and recreate text for sentiment analysis\n",
        "def custom_lemmatize_and_join(text):\n",
        "  text = (unicodedata.normalize('NFKD', text)\n",
        "         .encode('ascii', 'ignore')\n",
        "         .decode('utf-8', 'ignore')\n",
        "         .lower())\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word_list = text.split()        \n",
        "  text = ' '.join([lemmatizer.lemmatize(w) for w in word_list if not w in stop_words and len(w)>3])   \n",
        "  return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5X3isXeOi07"
      },
      "source": [
        "# Pre-process tweet\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()                                             # To lower\n",
        "    text = re.sub(r\"\\d+\", \" \", str(text))                           # Delete numbers\n",
        "    text = re.sub('&quot;',\" \", text)                               # Delete &quot;\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text)   # Delete link\n",
        "    text = re.sub('@[^\\s]+', '', text)                              # Delete user info\n",
        "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(text))                   # Delete one chars\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", str(text))                       # Delete punctuations\n",
        "    text = re.sub(r\"\\s+\", \" \", str(text))                           # Clean double space\n",
        "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)                         # Clean multiple repeats\n",
        "    for word in text.split():\n",
        "        if word.lower() in phrases:\n",
        "            text = text.replace(word, phrases[word.lower()])        # Swap common word phrases\n",
        "    custom_lemmatize_and_join(text)\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMdinWQUOtIx"
      },
      "source": [
        "news_df['preprocessed_title'] = news_df.title.apply(preprocess_text)\n",
        "news_df['preprocessed_article'] = news_df.article.apply(preprocess_text)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "58uh7flCPIqA",
        "outputId": "6ccda847-cab0-41e9-fc31-04cc1cea1da0"
      },
      "source": [
        "news_df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company_name</th>\n",
              "      <th>title</th>\n",
              "      <th>article</th>\n",
              "      <th>source</th>\n",
              "      <th>preprocessed_title</th>\n",
              "      <th>preprocessed_article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>apple</td>\n",
              "      <td>New York City Mayor-Elect Eric Adams says he w...</td>\n",
              "      <td>New York City Mayor-Elect Eric Adams took to T...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>new york city mayor elect eric adams says he w...</td>\n",
              "      <td>new york city mayor elect eric adams took to t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apple</td>\n",
              "      <td>Zia Credit Union review: Hispanic American-led...</td>\n",
              "      <td>Personal Finance Insider writes about products...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>zia credit union review hispanic american led ...</td>\n",
              "      <td>personal finance insider writes about products...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>apple</td>\n",
              "      <td>10 things in tech you need to know</td>\n",
              "      <td>Hello, world. We've got news about a new way t...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>things in tech you need to know</td>\n",
              "      <td>hello world we ve got news about new way to tw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>apple</td>\n",
              "      <td>5 ways I'm using credit card points to cover 7...</td>\n",
              "      <td>This post contains links to products from our ...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>ways using credit card points to cover of my ...</td>\n",
              "      <td>this post contains links to products from our ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>apple</td>\n",
              "      <td>Sen. Mitt Romney dresses up as Ted Lasso in Ha...</td>\n",
              "      <td>Sen. Mitt Romney of Utah donned one of this ye...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>sen mitt romney dresses up as ted lasso in hal...</td>\n",
              "      <td>sen mitt romney of utah donned one of this yea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>netflix</td>\n",
              "      <td>Hollywood Union Approves Contracts With Studio...</td>\n",
              "      <td>One of Hollywoods most powerful unions approve...</td>\n",
              "      <td>Bloomberg</td>\n",
              "      <td>hollywood union approves contracts with studio...</td>\n",
              "      <td>one of hollywoods most powerful unions approve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>netflix</td>\n",
              "      <td>South Korea's Moment Owes More to Samsung Than...</td>\n",
              "      <td>South Korea is having a moment. Between the hi...</td>\n",
              "      <td>Bloomberg</td>\n",
              "      <td>south korea moment owes more to samsung than s...</td>\n",
              "      <td>south korea is having moment between the hit n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>netflix</td>\n",
              "      <td>Disney+ is launching a special IMAX format for...</td>\n",
              "      <td>Disney+ is bringing IMAXs Expanded Aspect Rati...</td>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>disney is launching special imax format for ma...</td>\n",
              "      <td>disney is bringing imaxs expanded aspect ratio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>netflix</td>\n",
              "      <td>When Amazon Buys MGM, Hollywood Fears the Loss...</td>\n",
              "      <td>Where Hollywood and Silicon Valley collildeGet...</td>\n",
              "      <td>Bloomberg</td>\n",
              "      <td>when amazon buys mgm hollywood fears the loss ...</td>\n",
              "      <td>where hollywood and silicon valley collildeget...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>netflix</td>\n",
              "      <td>TechCrunch+ roundup: Yahoo leaves China, Nuban...</td>\n",
              "      <td>After Nubank filed its F-1, Natasha Mascarenha...</td>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>techcrunch roundup yahoo leaves china nubank i...</td>\n",
              "      <td>after nubank filed its natasha mascarenhas and...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>156 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    company_name  ...                               preprocessed_article\n",
              "0          apple  ...  new york city mayor elect eric adams took to t...\n",
              "1          apple  ...  personal finance insider writes about products...\n",
              "2          apple  ...  hello world we ve got news about new way to tw...\n",
              "3          apple  ...  this post contains links to products from our ...\n",
              "4          apple  ...  sen mitt romney of utah donned one of this yea...\n",
              "..           ...  ...                                                ...\n",
              "151      netflix  ...  one of hollywoods most powerful unions approve...\n",
              "152      netflix  ...  south korea is having moment between the hit n...\n",
              "153      netflix  ...  disney is bringing imaxs expanded aspect ratio...\n",
              "154      netflix  ...  where hollywood and silicon valley collildeget...\n",
              "155      netflix  ...  after nubank filed its natasha mascarenhas and...\n",
              "\n",
              "[156 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-0eaamvNdWE"
      },
      "source": [
        "vds = SentimentIntensityAnalyzer()\n",
        "\n",
        "def sentiment(data):\n",
        "    temp=[]\n",
        "    for row in data:\n",
        "        tmp=vds.polarity_scores(row)\n",
        "        temp.append(tmp)\n",
        "    return temp"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ydCQ4VfOXMP"
      },
      "source": [
        "news_df['VADER_title'] = sentiment(news_df['preprocessed_title'])\n",
        "news_df['VADER_article'] = sentiment(news_df['preprocessed_article'])\n",
        "news_df['title_compound']  = news_df['VADER_title'].apply(lambda score_dict: score_dict['compound'])\n",
        "news_df['article_compound']  = news_df['VADER_article'].apply(lambda score_dict: score_dict['compound'])\n",
        "news_df['title_sentiment']  = news_df['title_compound'].apply(lambda x: 2 if x > 0.05 else (0 if x < -0.05 else 1))\n",
        "news_df['article_sentiment']  = news_df['article_compound'].apply(lambda x: 2 if x > 0.05 else (0 if x < -0.05 else 1))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_9STHyIySVwb",
        "outputId": "6b988415-daf2-4c3e-d7aa-b11a4b3e17a9"
      },
      "source": [
        "news_df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company_name</th>\n",
              "      <th>title</th>\n",
              "      <th>article</th>\n",
              "      <th>source</th>\n",
              "      <th>preprocessed_title</th>\n",
              "      <th>preprocessed_article</th>\n",
              "      <th>VADER_title</th>\n",
              "      <th>VADER_article</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>article_compound</th>\n",
              "      <th>title_sentiment</th>\n",
              "      <th>article_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>apple</td>\n",
              "      <td>New York City Mayor-Elect Eric Adams says he w...</td>\n",
              "      <td>New York City Mayor-Elect Eric Adams took to T...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>new york city mayor elect eric adams says he w...</td>\n",
              "      <td>new york city mayor elect eric adams took to t...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apple</td>\n",
              "      <td>Zia Credit Union review: Hispanic American-led...</td>\n",
              "      <td>Personal Finance Insider writes about products...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>zia credit union review hispanic american led ...</td>\n",
              "      <td>personal finance insider writes about products...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.658, 'pos': 0.342, 'comp...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'comp...</td>\n",
              "      <td>0.6369</td>\n",
              "      <td>0.5346</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>apple</td>\n",
              "      <td>10 things in tech you need to know</td>\n",
              "      <td>Hello, world. We've got news about a new way t...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>things in tech you need to know</td>\n",
              "      <td>hello world we ve got news about new way to tw...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'comp...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>apple</td>\n",
              "      <td>5 ways I'm using credit card points to cover 7...</td>\n",
              "      <td>This post contains links to products from our ...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>ways using credit card points to cover of my ...</td>\n",
              "      <td>this post contains links to products from our ...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.675, 'pos': 0.325, 'comp...</td>\n",
              "      <td>{'neg': 0.054, 'neu': 0.946, 'pos': 0.0, 'comp...</td>\n",
              "      <td>0.6486</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>apple</td>\n",
              "      <td>Sen. Mitt Romney dresses up as Ted Lasso in Ha...</td>\n",
              "      <td>Sen. Mitt Romney of Utah donned one of this ye...</td>\n",
              "      <td>Business Insider</td>\n",
              "      <td>sen mitt romney dresses up as ted lasso in hal...</td>\n",
              "      <td>sen mitt romney of utah donned one of this yea...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.919, 'pos': 0.081, 'comp...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.4754</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>netflix</td>\n",
              "      <td>Hollywood Union Approves Contracts With Studio...</td>\n",
              "      <td>One of Hollywoods most powerful unions approve...</td>\n",
              "      <td>Bloomberg</td>\n",
              "      <td>hollywood union approves contracts with studio...</td>\n",
              "      <td>one of hollywoods most powerful unions approve...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'comp...</td>\n",
              "      <td>{'neg': 0.072, 'neu': 0.641, 'pos': 0.287, 'co...</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>0.8676</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>netflix</td>\n",
              "      <td>South Korea's Moment Owes More to Samsung Than...</td>\n",
              "      <td>South Korea is having a moment. Between the hi...</td>\n",
              "      <td>Bloomberg</td>\n",
              "      <td>south korea moment owes more to samsung than s...</td>\n",
              "      <td>south korea is having moment between the hit n...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.818, 'pos': 0.182, 'comp...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.7845</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>netflix</td>\n",
              "      <td>Disney+ is launching a special IMAX format for...</td>\n",
              "      <td>Disney+ is bringing IMAXs Expanded Aspect Rati...</td>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>disney is launching special imax format for ma...</td>\n",
              "      <td>disney is bringing imaxs expanded aspect ratio...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.56, 'pos': 0.44, 'compou...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.864, 'pos': 0.136, 'comp...</td>\n",
              "      <td>0.6705</td>\n",
              "      <td>0.5994</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>netflix</td>\n",
              "      <td>When Amazon Buys MGM, Hollywood Fears the Loss...</td>\n",
              "      <td>Where Hollywood and Silicon Valley collildeGet...</td>\n",
              "      <td>Bloomberg</td>\n",
              "      <td>when amazon buys mgm hollywood fears the loss ...</td>\n",
              "      <td>where hollywood and silicon valley collildeget...</td>\n",
              "      <td>{'neg': 0.345, 'neu': 0.541, 'pos': 0.115, 'co...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>-0.5267</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>netflix</td>\n",
              "      <td>TechCrunch+ roundup: Yahoo leaves China, Nuban...</td>\n",
              "      <td>After Nubank filed its F-1, Natasha Mascarenha...</td>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>techcrunch roundup yahoo leaves china nubank i...</td>\n",
              "      <td>after nubank filed its natasha mascarenhas and...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>156 rows Ã— 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    company_name  ... article_sentiment\n",
              "0          apple  ...                 1\n",
              "1          apple  ...                 2\n",
              "2          apple  ...                 2\n",
              "3          apple  ...                 0\n",
              "4          apple  ...                 2\n",
              "..           ...  ...               ...\n",
              "151      netflix  ...                 2\n",
              "152      netflix  ...                 2\n",
              "153      netflix  ...                 2\n",
              "154      netflix  ...                 1\n",
              "155      netflix  ...                 1\n",
              "\n",
              "[156 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYAfwA4ESXzW",
        "outputId": "15f87cf8-aedc-4fde-b36a-79ebd5d64c48"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "    \n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALmCRvPNUibO",
        "outputId": "e2cefae2-c1ef-4a6a-df39-8a299d1367be"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAh27X9nVCfQ"
      },
      "source": [
        "#FLAG: preprocessed_title or preprocessed_article\n",
        "flag = \"preprocessed_article\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wylm9sfMY62e",
        "outputId": "172e0351-1087-4f7f-faf8-514da352f6a6"
      },
      "source": [
        "sentiment = []\n",
        "train_sent = []\n",
        "\n",
        "for index, row in news_df.iterrows():\n",
        "  #print(f\"title_sentiment: {row['title_sentiment']}\")\n",
        "  #print(f\"preprocessed_title: {row['preprocessed_title']}\")\n",
        "  sentiment.append(row['article_sentiment'])\n",
        "  train_sent.append(row[flag])\n",
        "\n",
        "print(len(sentiment))\n",
        "print(len(train_sent))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156\n",
            "156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9eStHVDbpzo"
      },
      "source": [
        "sentence = train_sent\n",
        "binary = []\n",
        "\n",
        "for i in sentiment:\n",
        "    if i == 0:\n",
        "        binary.append(0)\n",
        "    elif i == 2:\n",
        "        binary.append(2)\n",
        "    else:\n",
        "        binary.append(1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I72q7ykDZQwr",
        "outputId": "a88bac9b-48f6-4054-c78b-0cf9c833586d"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentence[65])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentence[65]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence[65])))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  mark zuckerberg said it ridiculous for people to think that he changed his company facebook name to meta because of the recent wave of backlash the ceo told the verge in an interview which chars \n",
            "Tokenized:  ['mark', 'zu', '##cker', '##berg', 'said', 'it', 'ridiculous', 'for', 'people', 'to', 'think', 'that', 'he', 'changed', 'his', 'company', 'facebook', 'name', 'to', 'meta', 'because', 'of', 'the', 'recent', 'wave', 'of', 'backlash', 'the', 'ceo', 'told', 'the', 'verge', 'in', 'an', 'interview', 'which', 'char', '##s']\n",
            "Token IDs:  [2928, 16950, 9102, 4059, 2056, 2009, 9951, 2005, 2111, 2000, 2228, 2008, 2002, 2904, 2010, 2194, 9130, 2171, 2000, 18804, 2138, 1997, 1996, 3522, 4400, 1997, 25748, 1996, 5766, 2409, 1996, 16079, 1999, 2019, 4357, 2029, 25869, 2015]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn0savvBbOeS",
        "outputId": "d2f7a517-1746-4275-df25-3b32fb79ec74"
      },
      "source": [
        "max_len = 0\n",
        "# For every sentence...\n",
        "for sent in sentence:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COwyvfOAbhCI",
        "outputId": "ba3f6136-7c56-4ce8-9fb4-b0261807cf42"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentence:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 50,         # Pad & truncate all sentences.\n",
        "                        truncation = True,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(binary)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentence[65])\n",
        "print('Token IDs:', input_ids[65])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  mark zuckerberg said it ridiculous for people to think that he changed his company facebook name to meta because of the recent wave of backlash the ceo told the verge in an interview which chars \n",
            "Token IDs: tensor([  101,  2928, 16950,  9102,  4059,  2056,  2009,  9951,  2005,  2111,\n",
            "         2000,  2228,  2008,  2002,  2904,  2010,  2194,  9130,  2171,  2000,\n",
            "        18804,  2138,  1997,  1996,  3522,  4400,  1997, 25748,  1996,  5766,\n",
            "         2409,  1996, 16079,  1999,  2019,  4357,  2029, 25869,  2015,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z2ijhkQc7TX",
        "outputId": "a9e6f9e1-fc8f-4901-bfec-3eac19929b90"
      },
      "source": [
        "print(binary)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 2, 0, 2, 1, 2, 2, 2, 1, 2, 0, 1, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 0, 0, 2, 2, 1, 1, 2, 0, 1, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GbisOL3cfTG",
        "outputId": "34bee6cf-ef00-4f46-e828-66936c230fc6"
      },
      "source": [
        "labels = torch.tensor(np.array(binary))\n",
        "print(labels)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 2, 0, 2, 1, 2, 2, 2, 1, 2, 0, 1, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2,\n",
            "        0, 0, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 2, 2, 2, 1,\n",
            "        0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
            "        0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 2, 1,\n",
            "        2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 0, 0, 2, 2, 1, 1, 2, 0, 1, 0, 0,\n",
            "        2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqxWzMNHcm8Q",
        "outputId": "8fa27262-6185-499d-f767-50264b8d714f"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "print(len(input_ids),len(labels))\n",
        "\n",
        "\n",
        "# Shuffling the data \n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "input_ids, attention_masks, labels = shuffle(input_ids, attention_masks, labels, random_state=0)\n",
        "\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "split = 100\n",
        "test = 30\n",
        "val = 26\n",
        "train_dataset = TensorDataset(input_ids[:split], attention_masks[:split], labels[:split])\n",
        "val_dataset = TensorDataset(input_ids[split:split + val], attention_masks[split:split + val], labels[split:split + val])\n",
        "test_dataset = TensorDataset(input_ids[split + val:], attention_masks[split + val:], labels[split + val:])\n",
        "# Create a 90-10 train-validation split.\n",
        "print(len(test_dataset))\n",
        "print(labels[:split])\n",
        "\n",
        "p=0\n",
        "n=0\n",
        "neu=0\n",
        "b=0\n",
        "for l in labels[split:split+val]:\n",
        "  if l == 1:\n",
        "    neu+=1\n",
        "  elif l ==0:\n",
        "    n+=1\n",
        "  elif l==2:\n",
        "    p+=1\n",
        "  else:\n",
        "    b+=1\n",
        "print(neu,n,p,b)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156 156\n",
            "30\n",
            "tensor([2, 2, 1, 0, 0, 2, 2, 0, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 1, 0, 2, 2, 2, 1, 0,\n",
            "        2, 0, 2, 0, 2, 2, 1, 2, 0, 0, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2,\n",
            "        2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 1, 1, 1, 2, 0, 0,\n",
            "        2, 0, 2, 2])\n",
            "4 3 19 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-P4qdOydnzo"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(train_dataset), # Select batches sequentially\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP9Z-54DdrwD",
        "outputId": "b62d20fa-15f5-4a70-9697-28b316d1ad03"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBP6UsLiduCF"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 15\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJNSHGX3dyqk"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhZ5uJied1yA",
        "outputId": "1f71bf03-4cf8-4f53-f77f-7f1c3a80a9a8"
      },
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        print(type(loss))\n",
        "        print(loss)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(1.0343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(1.0279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(1.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.9678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation Loss: 0.84\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.8656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.9489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.9287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.8213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.89\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation Loss: 0.81\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.9142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.9026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.8294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.87\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation Loss: 0.77\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.8545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.8600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.75\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5071, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:09 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "6cVSkrOTd80f",
        "outputId": "ab2aef45-7032-4675-ab41-016deee01e5c"
      },
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.01</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.69</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.59</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               1.01         0.84           0.73       0:00:01         0:00:00\n",
              "2               0.89         0.81           0.73       0:00:01         0:00:00\n",
              "3               0.87         0.77           0.73       0:00:01         0:00:00\n",
              "4               0.81         0.72           0.73       0:00:01         0:00:00\n",
              "5               0.75         0.69           0.77       0:00:01         0:00:00\n",
              "6               0.69         0.70           0.81       0:00:01         0:00:00\n",
              "7               0.59         0.72           0.77       0:00:01         0:00:00\n",
              "8               0.52         0.69           0.81       0:00:01         0:00:00\n",
              "9               0.48         0.70           0.77       0:00:01         0:00:00\n",
              "10              0.44         0.70           0.77       0:00:01         0:00:00\n",
              "11              0.40         0.71           0.77       0:00:01         0:00:00\n",
              "12              0.37         0.71           0.77       0:00:01         0:00:00\n",
              "13              0.37         0.72           0.77       0:00:01         0:00:00\n",
              "14              0.34         0.72           0.77       0:00:01         0:00:00\n",
              "15              0.34         0.72           0.77       0:00:01         0:00:00"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "SqAppdrmflvy",
        "outputId": "2596cc01-cead-4556-e5b5-0d23e6bd7347"
      },
      "source": [
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAWxLa6Df0Kx",
        "outputId": "d74d58ea-8ec8-4d8b-89f5-be810b1d8352"
      },
      "source": [
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(news_df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = news_df.preprocessed_title.values\n",
        "labels = news_df.title_sentiment.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 156\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9y9wOg0f8cA",
        "outputId": "f665e655-baea-4de6-e238-350a0f924be0"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "prediction_dataloader = DataLoader(\n",
        "            test_dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Select batches sequentially\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 30 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZaJZAVbgmkN",
        "outputId": "55fb15f8-3a1e-4476-907a-91e64c0a64b7"
      },
      "source": [
        "print(len(true_labels))\n",
        "print(len(predictions))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifZ02hpUgN68",
        "outputId": "8fbf8cf7-16d9-4c82-b888-a7984e9cf1b7"
      },
      "source": [
        "pred_labels = []\n",
        "a = 0\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  pred_labels.append(np.argmax(predictions[i], axis=1).flatten())\n",
        "\n",
        "pred_labels = np.concatenate(pred_labels)\n",
        "true_labels = np.concatenate(true_labels)\n",
        "\n",
        "for i in range(len(pred_labels)):\n",
        "  if pred_labels[i] == true_labels[i]:\n",
        "    a += 1\n",
        "print('Accuracy : ', a/len(true_labels))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "rcqgLYfGgPZa",
        "outputId": "9df6aaa9-cfcc-478d-b79d-e4cbed0c4f5b"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (news_df.title_sentiment.sum(), len(news_df.title_sentiment), (news_df.title_sentiment.sum() / len(news_df.title_sentiment) * 100.0)))\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 199 of 156 (127.56%)\n",
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-73b6d5267826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Calculate and store the coef for this batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mmatthews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mmatthews_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatthews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmatthews_corrcoef\u001b[0;34m(y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m     \"\"\"\n\u001b[0;32m--> 887\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             raise TypeError(\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             )\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Singleton array 1 cannot be considered a valid collection."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ls4TM9biGoV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}